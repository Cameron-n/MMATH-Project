# -*- coding: utf-8 -*-
"""
Created on Thu Jan  7 11:49:32 2021

@author: Cameron
"""

# TRAINING SCRIPT
#---------------------------------------------------------------#
# Script for the main focus of my project.
# This contains all the data analysis tools, and requires data
# in the format generated by the Training.py script
# The tools available are as follows:
#
# 1-Plot the average gradients for a specific layer for all epochs
# 2-Plot the average gradients for a specific epoch for all layers
# 3-Create a gif of the above specific epoch/all layers for each epoch
# 4-Plot a scatter graph for average gradients for one dataset vs 
# average gradients for another dataset (for the same model)
# 5-Plot a heatmap for the last average gradients minus the first for
# each layer.
# 6-Calculate accuracy and f1-scores
#
# Could improve to automate some parts
#
#---------------------------------------------------------------#


import numpy as np
from matplotlib import pyplot as plt
import imageio

# Selects which directory to look for models
 
while True:
    filepath_dataset = input("Which dataset? ")
    if filepath_dataset == "c":
        filepath_dataset = "ImageNet_to_cifar10"
        break
    elif filepath_dataset == 'i':                   # Not needed now
        filepath_dataset = "cifar10_to_ImageNet"
        break
    elif filepath_dataset == 'm':
        filepath_dataset = "ImageNet_to_MNIST"
        break
    else:
        print("?")
        print("")

# Notes
        
# --data analysis--
        
# 1 - Confusion Matrix                                              -/
# All models have dog and cat most misclassed
# Then bird or deer
        
# 2 - 'Where' in image weights change most                          -/
# Last - first weights heatmap    

# 3 - Scattor plot dataset vs dataset                              easy,high=9(R)
        
# 4 - classification norm's                                         ongoing
        
# 5 - Other accuracy measures (f1-score etc)                        medi,mid=4

# ----------------
        
# --Training--

# 6 - Non-fine-tuning                                               -/
# x-axis nonfine epochs, y-axis accuracy
# 3/4 best at 1, std. for same total time
        
# 6(2) - nonfinetuning same finetuning (3D nonfine,fine,accuracy)   -/ (for [],[],[],[5])
        
# 7 - Train from scratch                                            medi,high=6 model low accuracy???
        
# 8 - % of data to use                                              medi,mid=4
        
# -----------
        
# 9 - cifar-100                                                     easy,low=3

# 10 - MNIST                                                        -/
# Similar gradient structure
        
# 11- List of things that didn't work/problems                      ongoing
# i.e cifar to imagenet, batchnorm freezing, 

### INFORMATION FOR USING THIS SCRIPT ###
# In progress
# Could add may to automatically detect fine and nonfine numbers
        
#CIFAR-10-------------------------------------------------#
# 0 - resnet50 - 18 , 41
# 1 - vgg
# 2 - vgg
# 3 - densenet121 - 36
# 4 - densenet169 - 37
# 5 - mobilenet_v2 - 19 (20,20) , 39 , 79 (1,50)
#---------------------------------------------------------#

#MNIST----------------------------------------------------#
# 0 - resnet50 - 117 (1,5)
# 1 - vgg
# 2 - vgg
# 3 - densenet121
# 4 - densenet169
# 5 - mobilenet_v2             
#---------------------------------------------------------#
        
available_model = [
        ['ResNet50'],
        ['VGG16'],
        ['VGG19'],
        ['DenseNet121'],
        ['DenseNet169'],
        ['mobilenet_v2']
        ]

# User input to select which model to use.
# Will currently only accept the six models above

while True:
    filepath_model = int(input("Which model? "))
    if filepath_model in range(6):
        filepath_model = available_model[filepath_model][0]
        break
    else:
        print("?")
        print("")

filepath_front = r"C:\Users\Cameron\MMath Project\Models"
filepath_middle = f"\\{filepath_dataset}\{filepath_model.lower()}\\" 

# Loads counter for the total number of models to stop selection
# of invalid model names

num = np.loadtxt(open(filepath_front+r'\num.txt','rb'), delimiter=",")
num = str(int(num[0]))

while True:
    answer = input("Choose which previous model? ")
    if answer == "":
        prev = num - 1
        break
    elif int(answer) in range(int(num)):
        prev = int(answer)
        break
    else:
        print("?")
        print("")

# Selects the location of the data based on user input
        
filepath_front = filepath_front+filepath_middle
filepath_back = [f"model_pretrained_{filepath_dataset}_{filepath_model}_history_{prev}",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_history_{prev}_f",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}_f",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}_weights",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}_f_weights"]



# Function definitions
###-----------------------------------------------------------###

# Crude function for plotting the average gradient size for any layer
# across all the epochs. Produces a graph of gradient size vs epoch.
# Can split the graph into a nonfine tuning and fine tuning section.
# Only selects layers by size, so distinct layers with the same shape
# will be merged onto one plot, producing multiple lines. This can be
# controlled by setting 'control' to 1 which allows the user to select
# which specific layer they wish to view.
# Will need to combine with possible_layers to select unique layer 
# numbers (or rework!)

def weight_matrix_norm_arbitrary_layer(num,partition,nonfine=20,fine=10,control=0):
    
    layer_weights = []
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    for layers in data:
        if layers.shape == data[num].shape:
            layer_weights.append(layers)
    
    count = 0
    limit = int((len(data)-nonfine)/(fine))
    
    for i in data[nonfine-1:nonfine-1+limit]:
        if i.shape==data[num].shape:
            count+=1
    
    counter = count
    if control == 1:        
        if count>1:
            print(count)
            skip = int(input("which layer? "))   
            count = [skip]
    else:
        count = range(count)
            
    for skip in count:
        thingy = layer_weights[skip::counter]
        thingy = np.diff(thingy,axis=0)
        if len(data[num].shape) == 2:
            thingy[0:nonfine-1] = thingy[0:nonfine-1]/0.001
            thingy[nonfine-1:] = thingy[nonfine-1:]/0.00001
        else:
            thingy = thingy/0.00001
        thingy = [np.linalg.norm(i) for i in thingy]
        thingy = thingy/(np.prod(data[num].shape))**(1/2) 
        
        #thingy = [(np.linalg.norm(i/max(np.max(i),-np.min(i))))/np.prod(i.shape) for i in thingy]
        #thingy = abs(np.diff(thingy))
        
        if partition != 1:
            plt.figure(4)
            plt.plot(thingy[:partition-1])#-thingy[-1]])
            plt.xlabel("Epoch")
            plt.ylabel("Norm of W_epoch+1 - W_epoch")
            plt.title("Before fine tuning")
        
        #x = [i for i in range(20)]
        plt.figure(5)
        plt.plot(thingy[partition-1:])#[:10])#-thingy[-1]])
        plt.xlabel("Epoch")
        plt.ylabel("Norm of W_epoch+1 - W_epoch")
        plt.title("After fine tuning")
    
    
# In contrast to above, graphs all layers' average gradients for
# a specified epoch. Can be combined with gif_builder to make gifs for
# all epochs by setting 'gifs' to True
        
def one_epoch_all_layers(num,nonfine=20,fine=10,gifs=False):
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    limit = int((len(data)-nonfine)/(fine))
    
    data_1 = data[nonfine:nonfine+limit]
    data_2 = data[nonfine+limit:nonfine+2*limit]
    
    data_m = [i - j for i,j in zip(data_1,data_2)]
##   Attempts to add animation for nonfine only
#    data_3 = data[0:nonfine]
#    data_3.append(data_1[-1])
#    data_3 = np.diff(data_3,axis=0)
    data_m = [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data_m]
#    data_3 = [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data_3]
#    data_m.append(data_3[num])
#    data_final = data_m
    
    maxdata = np.max(data_m)
    
    if num >= 0:
        data_1 = data[nonfine+num*limit:nonfine+(num+1)*limit]
        data_2 = data[nonfine+(num+1)*limit:nonfine+(num+2)*limit]
    
        data_final = [i - j for i,j in zip(data_1,data_2)]
        data_final = [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data_final]
        
    plt.plot(data_final) 
    plt.xlabel("Layer (0 = nearest to input)")
    plt.ylabel("Norm of W_epoch+1 - W_epoch")
    plt.title("Gradient vs layer")
    
    if gifs == True:
        plt.ylim(0,maxdata*1.1)
        plt.savefig(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_{num}')
        plt.close()
        
        
# Can make gifs out of the images produced by one_epoch_all_layers
# Requires that function to be run first to produce the images.
        
def gif_builder(length):
    
    filenames = []
    for j in range(5):
        filenames.append(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_0.png')
    for i in range(1,length):
        for j in range(1):
            filenames.append(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_{i}.png')
    for j in range(10):
        filenames.append(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_{length}.png')
        
    with imageio.get_writer(f'gifs//gif_{filepath_dataset}_{filepath_model}_all.gif', mode='I') as writer:
        for filename in filenames:
            image = imageio.imread(filename)
            writer.append_data(image)


# A shortened version of one_epoch_all_layers that saves only the
# average gradients (i.e. with no y-axis/layer identification).
# Standard use is to run the script on one dataset, then run this function
# and save its results to a variable. Then, rerun the script for another
# dataset and save the results in another variable. Then, use matplotlibs
# scatterplot function to graph a scatter plot.
            
def scatter_comp(num,nonfine=20,fine=10):
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    limit = int((len(data)-nonfine)/(fine))
    
    data_1 = data[nonfine+num*limit:nonfine+(num+1)*limit]
    data_2 = data[nonfine+(num+1)*limit:nonfine+(num+2)*limit]
    
    data = [i - j for i,j in zip(data_1,data_2)]
    
    return [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data]
    

# Produces a heatmap for the last gradients minus the first for any layer
# Restructured the mostly 4D weight matrices into 2D matrices by putting the
# 2D filters one after another.
    
def last_minus_first(num,nonfine=20,fine=10):
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    limit = int((len(data)-nonfine)/(fine))
    
    data_1 = data[nonfine-1:nonfine+limit]
    data_2 = data[-limit-1:]
    
    data = [i - j for i,j in zip(data_1,data_2)]
    
    s = data[num].shape
    shape = 1 + int(np.floor(np.sqrt(np.prod(s))))
    if len(s)==4:
        
        shape = 1+int(np.floor(np.sqrt(s[-2]*s[-1])))
        data = data[num].reshape(s[0],s[1],s[-2]*s[-1])
        new_zeros = np.zeros((s[0],s[1],shape**2-s[-2]*s[-1]))
        data = np.concatenate((data,new_zeros),2)
        data = data.reshape(s[0]*shape,s[1]*shape)
        
    else:
        
        data = data[num].flatten()
        new_zeros = np.zeros(shape**2-int(np.prod(s)))
        data = np.concatenate((data,new_zeros))
        data = data.reshape(shape,shape)
    
    plot = plt.matshow(data)
    plt.colorbar(plot)
    #plt.xlabel("Layer (0 = nearest to input)")
    #plt.ylabel("Norm of W_epoch+1 - W_epoch")
    plt.title(f"(Last - first) gradient heatmap for layer {num}",y=1.1)
    print(s)
    
    
# Ignore
    
def weight_matrix_whole_network(num,partition):
    pass
    
    
# Lists possible unique layers. i.e. those with distinct shapes.
# Only needed and useful for the crude functions that identify layers
# by their shape rather than more unique information.
    
def possible_layers(data):
    listy_1 = []
    listy_2 = []
    for i in range(len(data)):
        if len(data[i].shape)!=1 and data[i].shape not in listy_1:
            listy_1.append(data[i].shape)
            listy_2.append(i)
    return dict(zip(listy_1,listy_2))
        
    
# Calculates Top-1 accuracy for the given data
    
def accuracy(images=50000,classes=10,val=0.1,shape=30,cutoff=20,fine_tune=0):
    if fine_tune == 0:
        a = np.loadtxt(filepath_front+filepath_back[2])
    b = np.loadtxt(filepath_front+filepath_back[3])
    b = b[classes:]
    if fine_tune == 0:
        a = np.concatenate((a,b))
        b=a[classes:]
    c=b.reshape([shape,classes,classes])
    listy = []
    for i in range(len(c)):
        total = 0
        for j in range(len(c[0])):
            total+=c[i][j,j]
        listy.append(total/images)
    listy.insert(0,val)
    plt.figure(1)
    plt.plot(listy)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    plt.text(x=cutoff+1,y=listy[cutoff],s="fine tuning")
    
    new_listy=[]
    for i in range(len(listy)-1):
        new_listy.append(listy[i+1]-listy[i])
    plt.figure(2)
    plt.plot(new_listy,'bo')
    plt.xlabel('Epoch')
    plt.ylabel('Change in accuracy')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    new_listy_1 = new_listy[:cutoff]
    new_listy_2 = new_listy[cutoff:]
    
    nn_listy = []
    length_1 = len(new_listy_1)
    length_2 = len(new_listy_2)
    for num in new_listy_1:
        count = 0
        for ber in new_listy_2:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_2)
        
    for num in new_listy_2:
        count = 0
        for ber in new_listy_1:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_1)
    
    plt.figure(3)
    plt.plot(nn_listy,'rx')
    plt.xlabel('Epoch')
    plt.ylabel('Efficiency')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    plt.tight_layout()
    plt.show()
    
    
# Same as accuracy but calculates f1-score instead
    
def f1_score(images=50000,classes=10,val=0.1,shape=30,cutoff=20,fine_tune=0):
    if fine_tune == 0:
        a = np.loadtxt(filepath_front+filepath_back[2])
    b = np.loadtxt(filepath_front+filepath_back[3])
    b = b[classes:]
    if fine_tune == 0:
        a = np.concatenate((a,b))
        b=a[classes:]
    c=b.reshape([shape,classes,classes])
    listy = []
    for i in range(len(c)):
        avg = []
        for j in range(len(c[0])):
            tp = 0
            fp = 0
            fn = 0
            tp += c[i][j,j]
            for k in range(len(c[0])):
                fp += c[i][j,k]
                fn += c[i][k,j]
            fp -= tp
            fn -= tp
            total = tp/(tp+0.5*(fp+fn))
            avg.append(total)       
        listy.append(np.mean(avg))
        
    listy.insert(0,val)
    plt.figure(1)
    plt.plot(listy)
    plt.xlabel('Epoch')
    plt.ylabel('F-score')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    plt.text(x=cutoff+1,y=listy[cutoff],s="fine tuning")
    
    new_listy=[]
    for i in range(len(listy)-1):
        new_listy.append(listy[i+1]-listy[i])
    plt.figure(2)
    plt.plot(new_listy,'bo')
    plt.xlabel('Epoch')
    plt.ylabel('Change in F-score')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    new_listy_1 = new_listy[:cutoff]
    new_listy_2 = new_listy[cutoff:]
    
    nn_listy = []
    length_1 = len(new_listy_1)
    length_2 = len(new_listy_2)
    for num in new_listy_1:
        count = 0
        for ber in new_listy_2:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_2)
        
    for num in new_listy_2:
        count = 0
        for ber in new_listy_1:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_1)
    
    plt.figure(3)
    plt.plot(nn_listy,'rx')
    plt.xlabel('Epoch')
    plt.ylabel('Efficiency')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    plt.tight_layout()
    plt.show()

###-----------------------------------------------------------###
    

### Extra section for short commands ###
# Can be deleted 
#accuracy(shape=15,cutoff=0,fine_tune=1)
data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
data = [i for i in data if len(i.shape)>1]

# =============================================================================
# a = input("hi: ")
# b = input("hi: ")
# c = input ("hi: ")
# for i in range(a):
#     one_epoch_all_layers(a,nonfine=b,fine=c,gifs=True)
# gif_builder(a-1)
# =============================================================================
