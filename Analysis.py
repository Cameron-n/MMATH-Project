# -*- coding: utf-8 -*-
"""
Created on Thu Jan  7 11:49:32 2021

@author: Cameron
"""

# TRAINING SCRIPT
#---------------------------------------------------------------#
# Script for the main focus of my project.
# This contains all the data analysis tools, and requires data
# in the format generated by the Training.py script
# The tools available are as follows:
#
# 1-Plot the average gradients for a specific layer for all epochs
# 2-Plot the average gradients for a specific epoch for all layers
# 3-Create a gif of the above specific epoch/all layers for each epoch
# 4-Plot a scatter graph for average gradients for one dataset vs 
# average gradients for another dataset (for the same model)
# 5-Plot a heatmap for the last average gradients minus the first for
# each layer.
# 6-Calculate accuracy and f1-scores
# 7?-Confusion matrix viewer (In progress)
#
# Could improve to automate some parts
#
#---------------------------------------------------------------#


import numpy as np
from matplotlib import pyplot as plt
import imageio

# Selects which directory to look for models
 
while True:
    filepath_dataset = input("Which dataset? ")
    if filepath_dataset == "c":
        filepath_dataset = "ImageNet_to_cifar10"
        break
    elif filepath_dataset == 'i':                   # Not needed now
        filepath_dataset = "cifar10_to_ImageNet"
        break
    elif filepath_dataset == 'm':
        filepath_dataset = "ImageNet_to_MNIST"
        break
    else:
        print("?")
        print("")
        
available_model = [
        ['ResNet50'],
        ['VGG16'],
        ['VGG19'],
        ['DenseNet121'],
        ['DenseNet169'],
        ['mobilenet_v2']
        ]

# User input to select which model to use.
# Will currently only accept the six models above

while True:
    filepath_model = int(input("Which model? "))
    if filepath_model in range(6):
        filepath_model = available_model[filepath_model][0]
        break
    else:
        print("?")
        print("")

filepath_front = r"C:\Users\Cameron\MMath Project\Models" ## !! Replace with appropriate folder name
filepath_middle = f"\\{filepath_dataset}\{filepath_model.lower()}\\" 

# Loads counter for the total number of models to stop selection
# of invalid model names

num = np.loadtxt(open(filepath_front+r'\num.txt','rb'), delimiter=",")
num = str(int(num[0]))

while True:
    answer = input("Choose which previous model? ")
    if answer == "":
        prev = num - 1
        break
    elif int(answer) in range(int(num)):
        prev = int(answer)
        break
    else:
        print("?")
        print("")

# Selects the location of the data based on user input
        
filepath_front = filepath_front+filepath_middle
filepath_back = [f"model_pretrained_{filepath_dataset}_{filepath_model}_history_{prev}",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_history_{prev}_f",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}_f",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}_weights",
                 f"model_pretrained_{filepath_dataset}_{filepath_model}_cmatrix_{prev}_f_weights"]



# Function definitions
###-----------------------------------------------------------###

def weight_matrix_norm_arbitrary_layer(num,nonfine,fine,partition=1,control=0,save=False):
    """Create graph of a specified layer's weight gradients against epochs.

    num -- integer specifying layer. Call 'possible_layers' function for list of unique values.
    nonfine -- number of classification tuning epochs (during training)
    fine -- number of network tuning epochs (during training)
    partition -- Set's where red line should be for classification graph (see project)
    Should be replacable as this can be calculated from nonfine and fine.
    control -- crutch to select specific layer as the function searches for layers of
    the same size so it will merge multiple layers together. Set to 1 to select a
    specific line if the graph has multiple.
    save -- Set to True to save a png file.
    
    """
    layer_weights = []
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True) # load weights
    
    data = [i for i in data if len(i.shape)>1] # exclude biases
    
    for layers in data: # searches for layers of the same shape
        if layers.shape == data[num].shape:
            layer_weights.append(layers)
    
    count = 0
    limit = int((len(data)-nonfine)/(fine))
    
    for i in data[nonfine-1:nonfine-1+limit]: # detects different layers of same shape
        if i.shape==data[num].shape:
            count+=1
    
    counter = count
    if control == 1: # can be used to pick specific same shape layer        
        if count>1:
            print(count)
            skip = int(input("which layer? "))   
            count = [skip]
    else:
        count = range(count)
            
    for skip in count:
        layerWeights = layer_weights[skip::counter]
        layerWeights = np.diff(layerWeights,axis=0)
        if len(data[num].shape) == 2:
            layerWeights[0:nonfine-1] = layerWeights[0:nonfine-1]/0.001 # !! '0.001' corrosponds to Training.py learning rate. Need to change if different rate is used
            layerWeights[nonfine-1:] = layerWeights[nonfine-1:]/0.00001 # !! as above for fine/network tuning.
        else:
            layerWeights = layerWeights/0.00001 # !! as above for fine/network tuning
        layerWeights = [np.linalg.norm(i) for i in layerWeights]
        layerWeights = layerWeights/(np.prod(data[num].shape))**(1/2) 

        # Graph plotting
        
        if partition != 1 or True:
            plt.figure(4)
            plt.plot(layerWeights[:partition-1])
            plt.xlabel("Epoch")
            plt.ylabel("Average gradient")
            plt.title("Before fine tuning for classification layer")
        
        if num == 0:
            plt.figure()
            plt.plot(layerWeights)
            plt.xlabel("Epoch")
            plt.ylabel("Average gradient")
            plt.title("Before/After combined for classification layer")
            plt.axvline(x=partition-1, ls = "--", color="red", label="Fine-tuning starts")
            plt.legend()
            
            if save==True: # saves image
                plt.savefig(f"{filepath_model}_0_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
            plt.figure()
        
        plt.figure()
        plt.plot(layerWeights[partition-1:])
        plt.xlabel("Epoch")
        plt.ylabel("Norm of W_epoch+1 - W_epoch")
        plt.title(f"Gradient vs Epoch for layer {num-nonfine+1}")
    
    
# In contrast to above, graphs all layers' average gradients for
# a specified epoch. Can be combined with gif_builder to make gifs for
# all epochs by setting 'gifs' to True
        
def one_epoch_all_layers(num,nonfine=20,fine=10,gifs=False):
    """Graphs all layers average gradients for a specified epoch.
    Gif can be made for all epochs.

    num -- integer specifying epoch
    nonfine -- number of classification tuning epochs (during training)
    fine -- number of network tuning epochs (during training)
    gifs -- Set to True to save a png for gif building (see 'gif_builder')

    """
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    limit = int((len(data)-nonfine)/(fine))
    
    data_1 = data[nonfine:nonfine+limit]
    data_2 = data[nonfine+limit:nonfine+2*limit]
    
    data_m = [i - j for i,j in zip(data_1,data_2)]

    data_m = [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data_m]
    
    maxdata = np.max(data_m)
    
    if num >= 0:
        data_1 = data[nonfine+num*limit:nonfine+(num+1)*limit]
        data_2 = data[nonfine+(num+1)*limit:nonfine+(num+2)*limit]
    
        data_final = [i - j for i,j in zip(data_1,data_2)]
        data_final = [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data_final]
        
    #plt.figure()    
    p = plt.plot(data_final) 
    plt.xlabel("Layer (0 = nearest to input)")
    plt.ylabel("Average gradient")
    plt.title("Gradient vs layer")
    
    if gifs == True:
        plt.ylim(0,maxdata*1.1)
        plt.savefig(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_{num}')
        plt.close()
    return p
        
        
# Can make gifs out of the images produced by one_epoch_all_layers
# Requires that function to be run first to produce the images.
        
def gif_builder(length):
    """Makes gifs from 'one_epoch_all_layers' function. Requires that function to run
    first to produce images.

    length -- number of images in gif.

    Suggested usage would be to set up a loop to generate images and then call this
    function. This also eliminates the need to calculate length manually.

    """
    filenames = []
    for j in range(5): # '5' delays the start of the gif by repeating the first image.
        filenames.append(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_0.png')
    for i in range(1,length):
        for j in range(1):
            filenames.append(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_{i}.png')
    for j in range(10): # '10' delays end of the gif.
        filenames.append(f'gifs//gif_{filepath_dataset}_{filepath_model}_all_{length}.png')
        
    with imageio.get_writer(f'gifs//gif_{filepath_dataset}_{filepath_model}_all.gif', mode='I') as writer:
        for filename in filenames:
            image = imageio.imread(filename)
            writer.append_data(image)


# A shortened version of one_epoch_all_layers that saves only the
# average gradients (i.e. with no y-axis/layer identification).
# Standard use is to run the script on one dataset, then run this function
# and save its results to a variable. Then, rerun the script for another
# dataset and save the results in another variable. Then, use matplotlibs
# scatterplot function to graph a scatter plot.
            
def scatter_comp(num,nonfine=20,fine=10):
    """Plots scatter graph of one dataset's layer's weight gradients against anothers.

    num -- integer specifying epoch
    nonfine -- number of classification tuning layers (during training)
    fine -- number of network tuning layers (during training)

    Standard use: run script for first dataset. Run this function, saving to variable (i.e. a = scatter_comp)
    Run script for second dataset. Run this function, saving to variable (i.e. b = scatter_comp)
    Then plot a scatter graph between 'a' and 'b' manually.

    In-progress: improvement to automate the above.

    """
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    limit = int((len(data)-nonfine)/(fine))
    
    data_1 = data[nonfine+num*limit:nonfine+(num+1)*limit]
    data_2 = data[nonfine+(num+1)*limit:nonfine+(num+2)*limit]
    
    data = [i - j for i,j in zip(data_1,data_2)]
    
    return [np.linalg.norm(i)/(np.prod(i.shape))**(1/2) for i in data]
    

# Produces a heatmap for the last gradients minus the first for any layer
# Restructured the mostly 4D weight matrices into 2D matrices by putting the
# 2D filters one after another.
    
def last_minus_first(num,nonfine=20,fine=10):
    """Heatmap for last epoch's - first epoch's gradients for a specified layer.

    num -- integer specifying layer
    nonfine -- number of classification tuning epochs (during training)
    fine --  number of network tuning epochs (during training)

    In-progress: Need to work out how it organises the 2D picture from the 4D
    matrix. I think it stacks 2D filters one after another, so the graph should
    be blocks of nxn filters (usually n=3). But it does not look that way. Space
    is also added to make the graph square/rectangular.
    """
    data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    
    data = [i for i in data if len(i.shape)>1]
    
    limit = int((len(data)-nonfine)/(fine))
    
    data_1 = data[nonfine-1:nonfine+limit]
    data_2 = data[-limit-1:]
    
    data = [i - j for i,j in zip(data_1,data_2)]
    
    s = data[num].shape
    shape = 1 + int(np.floor(np.sqrt(np.prod(s))))
    if len(s)==4:
        
        shape = 1+int(np.floor(np.sqrt(s[-2]*s[-1])))
        data = data[num].reshape(s[0],s[1],s[-2]*s[-1])
        new_zeros = np.zeros((s[0],s[1],shape**2-s[-2]*s[-1]))
        data = np.concatenate((data,new_zeros),2)
        data = data.reshape(s[0]*shape,s[1]*shape)
        
    else:
        
        data = data[num].flatten()
        new_zeros = np.zeros(shape**2-int(np.prod(s)))
        data = np.concatenate((data,new_zeros))
        data = data.reshape(shape,shape)
    
    plot = plt.matshow(data)
    plt.colorbar(plot)
    if num == 0:
        plt.title(f"Last - first gradient heatmap for last layer",y=1.1)
    else:
        plt.title(f"Last - first gradient heatmap for layer {num}",y=1.1)
    print(s)
    
# Lists possible unique layers. i.e. those with distinct shapes.
# Only needed and useful for the crude functions that identify layers
# by their shape rather than more unique information.
    
def possible_layers(data=0):
    """Lists possible unique layers (up to shape).

    Data -- data containing weights.
    
    Will give a list of integers for use with 'weight_matrix_norm_arbitrary_layer' function.
    Another crutch due to only detecting layer shapes.
    """
    
    ## !! NOT TESTED below
    if data == 0:
        data = np.load(filepath_front+filepath_back[-1],allow_pickle=True)
    ## !! NOT TESTD above
    
    listy_1 = []
    listy_2 = []
    for i in range(len(data)):
        if len(data[i].shape)!=1 and data[i].shape not in listy_1:
            listy_1.append(data[i].shape)
            listy_2.append(i)
    return dict(zip(listy_1,listy_2))
        
    
# Calculates Top-1 accuracy for the given data
    
def accuracy(images=50000,classes=10,val=0.1,shape=30,cutoff=20,fine_tune=0,save=False):
    """Calculates Top-1 accuracy for the given data

    images -- number of images. default 50,000 (for use with CIFAR-10)
    classes -- number of classes.
    val --
    shape --
    cutoff --
    fine_tune --
    save --
    """
    
    if fine_tune == 0:
        a = np.loadtxt(filepath_front+filepath_back[2])
    b = np.loadtxt(filepath_front+filepath_back[3])
    b = b[classes:]
    if fine_tune == 0:
        a = np.concatenate((a,b))
        b=a[classes:]
    c=b.reshape([shape,classes,classes])
    listy = []
    for i in range(len(c)):
        total = 0
        for j in range(len(c[0])):
            total+=c[i][j,j]
        listy.append(total/images)
    listy.insert(0,val)
    plt.figure(1)
    plt.plot(listy)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    plt.text(x=cutoff+1,y=listy[cutoff],s="fine tuning")
    
    if save==True:
        plt.savefig(f"{filepath_model}_accuracy_1_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
    
    new_listy=[]
    for i in range(len(listy)-1):
        new_listy.append(listy[i+1]-listy[i])
    plt.figure(2)
    plt.plot(new_listy,'bo')
    plt.xlabel('Epoch')
    plt.ylabel('Change in accuracy')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    if save==True:
        plt.savefig(f"{filepath_model}_accuracy_2_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
    
    new_listy_1 = new_listy[:cutoff]
    new_listy_2 = new_listy[cutoff:]
    
    nn_listy = []
    length_1 = len(new_listy_1)
    length_2 = len(new_listy_2)
    for num in new_listy_1:
        count = 0
        for ber in new_listy_2:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_2)
        
    for num in new_listy_2:
        count = 0
        for ber in new_listy_1:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_1)
    
    plt.figure(3)
    plt.plot(nn_listy,'rx')
    plt.xlabel('Epoch')
    plt.ylabel('Efficiency')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    if save==True:
        plt.savefig(f"{filepath_model}_accuracy_3_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
    
    plt.tight_layout()
    plt.show()
    
    
# Same as accuracy but calculates f1-score instead
    
def f1_score(images=50000,classes=10,val=0.1,shape=30,cutoff=20,fine_tune=0,save=False):
    """Same as 'accuracy' function but for f1-score. See 'accuracy' for information."""
    
    if fine_tune == 0:
        a = np.loadtxt(filepath_front+filepath_back[2])
    b = np.loadtxt(filepath_front+filepath_back[3])
    b = b[classes:]
    if fine_tune == 0:
        a = np.concatenate((a,b))
        b=a[classes:]
    c=b.reshape([shape,classes,classes])
    listy = []
    for i in range(len(c)):
        avg = []
        for j in range(len(c[0])):
            tp = 0
            fp = 0
            fn = 0
            tp += c[i][j,j]
            for k in range(len(c[0])):
                fp += c[i][j,k]
                fn += c[i][k,j]
            fp -= tp
            fn -= tp
            total = tp/(tp+0.5*(fp+fn))
            avg.append(total)       
        listy.append(np.mean(avg))
        
    listy.insert(0,val)
    plt.figure(1)
    plt.plot(listy)
    plt.xlabel('Epoch')
    plt.ylabel('F-score')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    plt.text(x=cutoff+1,y=listy[cutoff],s="fine tuning")
    
    if save==True:
        plt.savefig(f"{filepath_model}_f1score_1_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
    
    new_listy=[]
    for i in range(len(listy)-1):
        new_listy.append(listy[i+1]-listy[i])
    plt.figure(2)
    plt.plot(new_listy,'bo')
    plt.xlabel('Epoch')
    plt.ylabel('Change in F-score')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    if save==True:
        plt.savefig(f"{filepath_model}_f1score_2_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
    
    new_listy_1 = new_listy[:cutoff]
    new_listy_2 = new_listy[cutoff:]
    
    nn_listy = []
    length_1 = len(new_listy_1)
    length_2 = len(new_listy_2)
    for num in new_listy_1:
        count = 0
        for ber in new_listy_2:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_2)
        
    for num in new_listy_2:
        count = 0
        for ber in new_listy_1:
            if num>=ber:
                count+=1
        nn_listy.append(count/length_1)
    
    plt.figure(3)
    plt.plot(nn_listy,'rx')
    plt.xlabel('Epoch')
    plt.ylabel('Efficiency')
    plt.axvline(x=cutoff,color='green',linestyle='--')
    
    if save==True:
        plt.savefig(f"{filepath_model}_f1score_3_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
    
    plt.tight_layout()
    plt.show()

###-----------------------------------------------------------###
    

### Extra section for short commands ###

data = np.load(filepath_front+filepath_back[-1],allow_pickle=True) # loads data
data = [i for i in data if len(i.shape)>1] # removes biases

# ============================================================================= Example gif building code
# a = input("")
# b = input("")
# c = input("")
# for i in range(a):
#     one_epoch_all_layers(a,nonfine=b,fine=c,gifs=True)
# gif_builder(a-1)
# =============================================================================

## Example use of the script for generating images. Improvable by making this into a module
## and creating another script to access this one.
## Can delete below.
 
#images = 50000
#nonfine = 20
#K = [0,1,2,3]
#partition = 1
#p = list(possible_layers(data).values())
#if filepath_dataset == "ImageNet_to_MNIST":
#    nonfine = 20#1
#    fine = 20#5
#    images = 60000
#    L = [0,8]#[0,3]
#    partition = 20#1
#elif filepath_model == "mobilenet_v2":
#    fine = 20
#    L = [0,18]
#    partition = 20
#else:
#    fine = 10
#    L = [0,8]
#    partition = 20

#weight_matrix_norm_arbitrary_layer(p[0],partition,nonfine,fine,control=0,save=True)
#plt.savefig(f"{filepath_model}_0_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
#for num in p[1::20]:    
#    weight_matrix_norm_arbitrary_layer(num,partition=1,nonfine=nonfine,fine=fine,control=0)
#    plt.figure()
#for num in p[1::5]:
#    weight_matrix_norm_arbitrary_layer(num,partition=1,nonfine=nonfine,fine=fine,control=0)
#    plt.savefig(f"{filepath_model}_{num}_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
#plt.figure()
#temp=[]
#for num in L:    
#    line_num = one_epoch_all_layers(num,nonfine,fine,gifs=False)
#    temp.append(line_num[0])
#plt.legend(temp,['First epoch','Last epoch'])
#plt.savefig(f"{filepath_model}_firstLast_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
#for num in K:
#    last_minus_first(num,nonfine,fine)
#    plt.savefig(f"{filepath_model}_heatmap_{num}_{filepath_dataset}.png",dpi=300,bbox_inches='tight')
#accuracy(images,classes=10,val=0.1,shape=nonfine+fine,cutoff=nonfine,fine_tune=0,save=True)
#f1_score(images,classes=10,val=0.1,shape=nonfine+fine,cutoff=nonfine,fine_tune=0,save=True)

#d1 = d1/np.max(d1)
#d2 = d2/np.max(d2)
#plt.scatter(d1,d2)
#plt.gca().set_aspect('equal', adjustable='box')
#plt.plot([0,1],[0,1],ls='--')

#axes = plt.gca()
#axes.set_xlim([0,np.max(d1)])
#axes.set_ylim([0,np.max(d2)])
#plt.scatter(d1,d2)
#plt.ticklabel_format(axis="x",style="sci",scilimits=(0,0))
#plt.ticklabel_format(axis="y",style="sci",scilimits=(0,0))
#plt.plot([0,1],[0,1],ls='--')
#plt.ylabel("MNIST")
#plt.xlabel("CIFAR-10")
#plt.savefig(f"{filepath_model}_scatter_{filepath_dataset}.png",dpi=300)
